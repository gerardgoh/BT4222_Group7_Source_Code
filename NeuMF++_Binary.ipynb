{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0wEniY01Bq",
        "outputId": "d9b4471b-7d29-4307-bfc0-f73539d764cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: SETUP AND DATA LOADING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Load data\n",
        "reviews_df = pd.read_csv(\"/content/drive/My Drive/bt4222data/Reviews Data Cleaned/cleaned_reviews.csv\", keep_default_na=False)\n",
        "metadata_df = pd.read_csv(\"/content/drive/My Drive/bt4222data/Meta Data Cleaned/final_metadata_cleaned.csv\", keep_default_na=False)\n",
        "\n",
        "\n",
        "# Print basic info about the datasets\n",
        "print(f\"Reviews shape: {reviews_df.shape}\")\n",
        "print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Filter to common ASINs if needed\n",
        "common_asins = set(reviews_df['asin']).intersection(set(metadata_df['asin']))\n",
        "print(f\"Common ASINs: {len(common_asins)}\")\n",
        "\n",
        "reviews_df = reviews_df[reviews_df['asin'].isin(common_asins)]\n",
        "metadata_df = metadata_df[metadata_df['asin'].isin(common_asins)]\n",
        "\n",
        "# Convert ratings to binary (1 if rating >= 3, else 0)\n",
        "reviews_df['interaction'] = (reviews_df['overall'] >= 3).astype(int)\n",
        "\n",
        "# Display binary interaction distribution\n",
        "print(f\"Interaction distribution (binary):\\n{reviews_df['interaction'].value_counts()}\")\n",
        "\n",
        "# Encode user and item IDs\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "reviews_df['user_idx'] = user_encoder.fit_transform(reviews_df['reviewerID'])\n",
        "reviews_df['item_idx'] = item_encoder.fit_transform(reviews_df['asin'])\n",
        "\n",
        "# Create a mapping from asin to item_idx for later use\n",
        "asin_to_idx = dict(zip(reviews_df['asin'], reviews_df['item_idx']))\n",
        "\n",
        "# Add item_idx to metadata_df\n",
        "metadata_df['item_idx'] = metadata_df['asin'].map(asin_to_idx)\n",
        "\n",
        "# Fill any NaN in description with empty string\n",
        "metadata_df['description'] = metadata_df['description'].fillna('')\n",
        "metadata_df['title'] = metadata_df['title'].fillna('')\n",
        "\n",
        "print(f\"Unique users: {reviews_df['reviewerID'].nunique()}\")\n",
        "print(f\"Unique items: {reviews_df['asin'].nunique()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: LOAD PRECOMPUTED BERT EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the precomputed BERT embeddings\n",
        "bert_embeddings_path = \"/content/drive/My Drive/bt4222data/embeddings/metadata_embeddings.npz\"\n",
        "print(f\"Loading precomputed BERT embeddings from {bert_embeddings_path}\")\n",
        "\n",
        "try:\n",
        "    loaded_data = np.load(bert_embeddings_path)\n",
        "    # Depending on how the embeddings were saved, you may need to adjust this\n",
        "    if 'embeddings' in loaded_data:\n",
        "        bert_embeddings = loaded_data['embeddings']\n",
        "    else:\n",
        "        bert_embeddings = loaded_data['arr_0']  # Default key when saving with np.savez\n",
        "\n",
        "    print(f\"Loaded BERT embeddings with shape: {bert_embeddings.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading BERT embeddings: {e}\")\n",
        "    raise\n",
        "\n",
        "# Ensure metadata is sorted by item_idx\n",
        "metadata_df = metadata_df.sort_values('item_idx').reset_index(drop=True)\n",
        "\n",
        "# If necessary, verify that the embedding order matches the sorted metadata\n",
        "print(f\"Number of items with embeddings: {len(bert_embeddings)}\")\n",
        "print(f\"Number of items in metadata: {len(metadata_df)}\")\n",
        "\n",
        "if len(bert_embeddings) != len(metadata_df):\n",
        "    print(\"Warning: BERT embeddings count does not match metadata count. Adjusting...\")\n",
        "    # This might need more sophisticated handling depending on your data\n",
        "\n",
        "# Get the embedding dimension\n",
        "bert_embedding_dim = bert_embeddings.shape[1]\n",
        "print(f\"BERT embedding dimension: {bert_embedding_dim}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: AUTOENCODER FOR FEATURE EXTRACTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class StackedDenoisingAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked Denoising Autoencoder (SDAE) for extracting latent features from BERT embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.2):\n",
        "        super(StackedDenoisingAutoencoder, self).__init__()\n",
        "\n",
        "        # Ensure hidden_dims is a list\n",
        "        if not isinstance(hidden_dims, list):\n",
        "            hidden_dims = [hidden_dims]\n",
        "\n",
        "        # Create encoder layers\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "        for i, dim in enumerate(hidden_dims):\n",
        "            encoder_layers.append(nn.Linear(prev_dim, dim))\n",
        "            if i < len(hidden_dims) - 1:  # No activation on bottleneck layer\n",
        "                encoder_layers.append(nn.BatchNorm1d(dim))\n",
        "                encoder_layers.append(nn.ReLU())\n",
        "                encoder_layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = dim\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Create decoder layers (in reverse)\n",
        "        decoder_layers = []\n",
        "        prev_dim = hidden_dims[-1]  # Start from bottleneck\n",
        "        for i, dim in enumerate(reversed(hidden_dims[:-1])):  # Skip bottleneck\n",
        "            decoder_layers.append(nn.Linear(prev_dim, dim))\n",
        "            decoder_layers.append(nn.BatchNorm1d(dim))\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            decoder_layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = dim\n",
        "\n",
        "        # Final reconstruction layer\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def add_noise(self, x, noise_factor=0.2):\n",
        "        \"\"\"Add Gaussian noise to the input\"\"\"\n",
        "        noisy_x = x + noise_factor * torch.randn_like(x)\n",
        "        return noisy_x\n",
        "\n",
        "    def forward(self, x, add_noise=True):\n",
        "        # Add noise for denoising effect during training\n",
        "        if add_noise and self.training:\n",
        "            x = self.add_noise(x)\n",
        "\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "        # Decode\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return decoded, encoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Get the latent representation\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "# Define SDAE for GMF and MLP\n",
        "GMF_HIDDEN_DIMS = [128, 64, 32, 16, 8]  # Latent dim is 8\n",
        "MLP_HIDDEN_DIMS = [256, 128, 64, 32, 16, 8]  # Latent dim is 8\n",
        "\n",
        "# Define and train SDAEs\n",
        "def train_autoencoder(autoencoder, input_data, batch_size=64, epochs=10, lr=0.001, weight_decay=1e-5):\n",
        "    \"\"\"Train the autoencoder\"\"\"\n",
        "    autoencoder.to(device)\n",
        "    autoencoder.train()\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    tensor_data = torch.tensor(input_data, dtype=torch.float).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(tensor_data)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            x = batch[0]\n",
        "\n",
        "            # Forward\n",
        "            reconstructed, _ = autoencoder(x)\n",
        "            loss = criterion(reconstructed, x)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * len(x)\n",
        "\n",
        "        avg_loss = total_loss / len(tensor_data)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Initialize GMF and MLP SDAEs\n",
        "gmf_sdae = StackedDenoisingAutoencoder(bert_embedding_dim, GMF_HIDDEN_DIMS)\n",
        "mlp_sdae = StackedDenoisingAutoencoder(bert_embedding_dim, MLP_HIDDEN_DIMS)\n",
        "\n",
        "# Check if saved models exist\n",
        "MODELS_DIR = \"models/\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "GMF_SDAE_PATH = os.path.join(MODELS_DIR, \"gmf_sdae.pth\")\n",
        "MLP_SDAE_PATH = os.path.join(MODELS_DIR, \"mlp_sdae.pth\")\n",
        "\n",
        "# Train or load GMF SDAE\n",
        "if os.path.exists(GMF_SDAE_PATH):\n",
        "    print(f\"Loading pretrained GMF SDAE from {GMF_SDAE_PATH}\")\n",
        "    gmf_sdae.load_state_dict(torch.load(GMF_SDAE_PATH, map_location=device))\n",
        "else:\n",
        "    print(\"Training GMF SDAE...\")\n",
        "    gmf_losses = train_autoencoder(gmf_sdae, bert_embeddings, batch_size=128, epochs=10)\n",
        "    torch.save(gmf_sdae.state_dict(), GMF_SDAE_PATH)\n",
        "\n",
        "# Train or load MLP SDAE\n",
        "if os.path.exists(MLP_SDAE_PATH):\n",
        "    print(f\"Loading pretrained MLP SDAE from {MLP_SDAE_PATH}\")\n",
        "    mlp_sdae.load_state_dict(torch.load(MLP_SDAE_PATH, map_location=device))\n",
        "else:\n",
        "    print(\"Training MLP SDAE...\")\n",
        "    mlp_losses = train_autoencoder(mlp_sdae, bert_embeddings, batch_size=128, epochs=10)\n",
        "    torch.save(mlp_sdae.state_dict(), MLP_SDAE_PATH)\n",
        "\n",
        "# Move to CPU for feature extraction\n",
        "gmf_sdae.to('cpu')\n",
        "mlp_sdae.to('cpu')\n",
        "gmf_sdae.eval()\n",
        "mlp_sdae.eval()\n",
        "\n",
        "# Extract latent features\n",
        "with torch.no_grad():\n",
        "    bert_tensor = torch.tensor(bert_embeddings, dtype=torch.float)\n",
        "    gmf_item_features = gmf_sdae.encode(bert_tensor).numpy()\n",
        "    mlp_item_features = mlp_sdae.encode(bert_tensor).numpy()\n",
        "\n",
        "print(f\"GMF item features shape: {gmf_item_features.shape}\")\n",
        "print(f\"MLP item features shape: {mlp_item_features.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: DATASET AND DATALOADER CREATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class NeuMFPlusPlusDataset(Dataset):\n",
        "    def __init__(self, interactions_df, gmf_item_features, mlp_item_features):\n",
        "        self.users = torch.tensor(interactions_df['user_idx'].values, dtype=torch.long)\n",
        "        self.items = torch.tensor(interactions_df['item_idx'].values, dtype=torch.long)\n",
        "        self.labels = torch.tensor(interactions_df['interaction'].values, dtype=torch.float)\n",
        "        self.gmf_item_features = torch.tensor(gmf_item_features, dtype=torch.float)\n",
        "        self.mlp_item_features = torch.tensor(mlp_item_features, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_id = self.users[idx]\n",
        "        item_id = self.items[idx]\n",
        "        label = self.labels[idx]\n",
        "        gmf_item_feature = self.gmf_item_features[item_id]\n",
        "        mlp_item_feature = self.mlp_item_features[item_id]\n",
        "\n",
        "        return user_id, item_id, gmf_item_feature, mlp_item_feature, label\n",
        "\n",
        "# Train/validation/test split using stratified sampling\n",
        "train_df, temp_df = train_test_split(\n",
        "    reviews_df, test_size=0.3, random_state=seed,\n",
        "    stratify=reviews_df['interaction']\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=seed,\n",
        "    stratify=temp_df['interaction']\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "train_dataset = NeuMFPlusPlusDataset(train_df, gmf_item_features, mlp_item_features)\n",
        "val_dataset = NeuMFPlusPlusDataset(val_df, gmf_item_features, mlp_item_features)\n",
        "test_dataset = NeuMFPlusPlusDataset(test_df, gmf_item_features, mlp_item_features)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: MODEL DEFINITION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class NeuMFPlusPlus(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Matrix Factorization Plus Plus (NeuMF++) model with separate feature extraction\n",
        "    for GMF and MLP components, incorporating the learned latent features from BERT embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 embedding_dim=8,\n",
        "                 gmf_feature_dim=8,\n",
        "                 mlp_feature_dim=8,\n",
        "                 mlp_dims=[32, 16, 8],\n",
        "                 dropout_rate=0.2):\n",
        "        super(NeuMFPlusPlus, self).__init__()\n",
        "\n",
        "        # GMF part\n",
        "        self.user_gmf_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_gmf_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # MLP part\n",
        "        self.user_mlp_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_mlp_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # Batch normalization for feature embeddings\n",
        "        self.gmf_feature_bn = nn.BatchNorm1d(gmf_feature_dim)\n",
        "        self.mlp_feature_bn = nn.BatchNorm1d(mlp_feature_dim)\n",
        "\n",
        "        # MLP layers\n",
        "        mlp_input_dim = embedding_dim * 2 + mlp_feature_dim  # user + item + feature\n",
        "        self.mlp_layers = nn.ModuleList()\n",
        "        self.mlp_batch_norms = nn.ModuleList()\n",
        "\n",
        "        # First layer\n",
        "        self.mlp_layers.append(nn.Linear(mlp_input_dim, mlp_dims[0]))\n",
        "        self.mlp_batch_norms.append(nn.BatchNorm1d(mlp_dims[0]))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(mlp_dims)-1):\n",
        "            self.mlp_layers.append(nn.Linear(mlp_dims[i], mlp_dims[i+1]))\n",
        "            self.mlp_batch_norms.append(nn.BatchNorm1d(mlp_dims[i+1]))\n",
        "\n",
        "        # Output layer\n",
        "        # GMF part: embedding_dim + feature_dim\n",
        "        # MLP part: mlp_dims[-1]\n",
        "        self.output_layer = nn.Linear(embedding_dim + gmf_feature_dim + mlp_dims[-1], 1)\n",
        "\n",
        "        # Activation and dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize embeddings with normal distribution\n",
        "        nn.init.normal_(self.user_gmf_embedding.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_gmf_embedding.weight, std=0.01)\n",
        "        nn.init.normal_(self.user_mlp_embedding.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_mlp_embedding.weight, std=0.01)\n",
        "\n",
        "        # Initialize linear layers with Xavier/Glorot\n",
        "        for layer in self.mlp_layers:\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
        "        nn.init.zeros_(self.output_layer.bias)\n",
        "\n",
        "    def forward(self, user_indices, item_indices, gmf_item_features, mlp_item_features):\n",
        "        # GMF part\n",
        "        user_gmf_emb = self.user_gmf_embedding(user_indices)\n",
        "        item_gmf_emb = self.item_gmf_embedding(item_indices)\n",
        "\n",
        "        # Process GMF item features\n",
        "        gmf_item_features = self.gmf_feature_bn(gmf_item_features)\n",
        "\n",
        "        # Element-wise product for GMF\n",
        "        # Combine embedding with features\n",
        "        gmf_vector = torch.cat([user_gmf_emb * item_gmf_emb, gmf_item_features], dim=1)\n",
        "\n",
        "        # MLP part\n",
        "        user_mlp_emb = self.user_mlp_embedding(user_indices)\n",
        "        item_mlp_emb = self.item_mlp_embedding(item_indices)\n",
        "\n",
        "        # Process MLP item features\n",
        "        mlp_item_features = self.mlp_feature_bn(mlp_item_features)\n",
        "\n",
        "        # Concatenate for MLP\n",
        "        mlp_input = torch.cat([user_mlp_emb, item_mlp_emb, mlp_item_features], dim=1)\n",
        "\n",
        "        # Apply MLP layers\n",
        "        for i, layer in enumerate(self.mlp_layers):\n",
        "            mlp_input = layer(mlp_input)\n",
        "            mlp_input = self.mlp_batch_norms[i](mlp_input)\n",
        "            mlp_input = self.relu(mlp_input)\n",
        "            mlp_input = self.dropout(mlp_input)\n",
        "\n",
        "        # Concatenate GMF and MLP parts\n",
        "        concat_output = torch.cat([gmf_vector, mlp_input], dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        prediction = self.sigmoid(self.output_layer(concat_output))\n",
        "\n",
        "        return prediction.squeeze()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: TRAINING AND EVALUATION FUNCTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train model for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    train_bar = tqdm(train_loader, desc=\"Training\")\n",
        "    for user_ids, item_ids, gmf_item_features, mlp_item_features, labels in train_bar:\n",
        "        # Move tensors to device\n",
        "        user_ids = user_ids.to(device)\n",
        "        item_ids = item_ids.to(device)\n",
        "        gmf_item_features = gmf_item_features.to(device)\n",
        "        mlp_item_features = mlp_item_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(user_ids, item_ids, gmf_item_features, mlp_item_features)\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "        train_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_ids, item_ids, gmf_item_features, mlp_item_features, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            # Move tensors to device\n",
        "            user_ids = user_ids.to(device)\n",
        "            item_ids = item_ids.to(device)\n",
        "            gmf_item_features = gmf_item_features.to(device)\n",
        "            mlp_item_features = mlp_item_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(user_ids, item_ids, gmf_item_features, mlp_item_features)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item() * len(labels)\n",
        "\n",
        "            # Store predictions and labels\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Convert predictions to binary (threshold = 0.5)\n",
        "    binary_preds = (all_preds >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    precision = precision_score(all_labels, binary_preds)\n",
        "    recall = recall_score(all_labels, binary_preds)\n",
        "    f1 = f1_score(all_labels, binary_preds)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "def evaluate_cold_start_items(model, all_df, test_df, gmf_item_features, mlp_item_features, min_interactions=5):\n",
        "    \"\"\"\n",
        "    Evaluate model specifically on cold start items from the test set.\n",
        "    Cold start items are defined as items with minimal user interactions.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating model on cold start items...\")\n",
        "\n",
        "    # Count interactions per item in the full dataset\n",
        "    item_counts = all_df['asin'].value_counts()\n",
        "\n",
        "    # Find cold start items that appear in the test set\n",
        "    cold_start_items = set(item_counts[item_counts <= min_interactions].index) & set(test_df['asin'].unique())\n",
        "\n",
        "    if not cold_start_items:\n",
        "        print(f\"No cold start items found with <= {min_interactions} interactions in the test set.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found {len(cold_start_items)} cold start items in the test set.\")\n",
        "\n",
        "    # Filter test data to only include cold start items\n",
        "    cold_start_test_df = test_df[test_df['asin'].isin(cold_start_items)]\n",
        "\n",
        "    if len(cold_start_test_df) == 0:\n",
        "        print(\"No data available for cold start evaluation.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Number of interactions with cold start items: {len(cold_start_test_df)}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    cold_start_dataset = NeuMFPlusPlusDataset(cold_start_test_df, gmf_item_features, mlp_item_features)\n",
        "    cold_start_loader = DataLoader(cold_start_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Evaluate using the same criterion\n",
        "    criterion = nn.BCELoss()\n",
        "    cold_start_metrics = evaluate(model, cold_start_loader, criterion, device)\n",
        "\n",
        "    print(\"Cold Start Items Metrics:\")\n",
        "    print(f\"Loss: {cold_start_metrics['loss']:.4f}\")\n",
        "    print(f\"Accuracy: {cold_start_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {cold_start_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {cold_start_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {cold_start_metrics['f1']:.4f}\")\n",
        "    print(f\"AUC: {cold_start_metrics['auc']:.4f}\")\n",
        "\n",
        "    return cold_start_metrics\n",
        "\n",
        "def generate_recommendations(model, user_encoder, item_encoder, all_df, test_df, gmf_item_features, mlp_item_features, n_recommendations=5):\n",
        "    \"\"\"\n",
        "    Generate n recommendations for users in the test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get unique users from test set\n",
        "    test_users = test_df['reviewerID'].unique()\n",
        "\n",
        "    # Get all items\n",
        "    all_items = all_df['asin'].unique()\n",
        "\n",
        "    # Create tensors of all item features\n",
        "    gmf_item_features_tensor = torch.tensor(gmf_item_features, dtype=torch.float)\n",
        "    mlp_item_features_tensor = torch.tensor(mlp_item_features, dtype=torch.float)\n",
        "\n",
        "    recommendations = {}\n",
        "\n",
        "    for user in tqdm(test_users[:10], desc=\"Generating recommendations\"):  # Just do 10 users for demo\n",
        "        user_idx = user_encoder.transform([user])[0]\n",
        "\n",
        "        # Get items the user hasn't interacted with yet\n",
        "        user_items = all_df[all_df['reviewerID'] == user]['asin'].values\n",
        "        unseen_items = np.setdiff1d(all_items, user_items)\n",
        "\n",
        "        if len(unseen_items) == 0:\n",
        "            print(f\"User {user} has interacted with all items!\")\n",
        "            continue\n",
        "\n",
        "        # Convert unseen items to indices\n",
        "        unseen_item_indices = torch.tensor([asin_to_idx[asin] for asin in unseen_items if asin in asin_to_idx], dtype=torch.long)\n",
        "\n",
        "        if len(unseen_item_indices) == 0:\n",
        "            print(f\"No valid unseen items for user {user}\")\n",
        "            continue\n",
        "\n",
        "        # Get features for unseen items\n",
        "        gmf_item_feat = gmf_item_features_tensor[unseen_item_indices]\n",
        "        mlp_item_feat = mlp_item_features_tensor[unseen_item_indices]\n",
        "\n",
        "        # Process in batches to avoid memory issues\n",
        "        batch_size = 1024\n",
        "        all_scores = []\n",
        "\n",
        "        for i in range(0, len(unseen_item_indices), batch_size):\n",
        "            batch_indices = unseen_item_indices[i:i+batch_size]\n",
        "            batch_gmf_features = gmf_item_feat[i:i+batch_size]\n",
        "            batch_mlp_features = mlp_item_feat[i:i+batch_size]\n",
        "\n",
        "            user_tensor = torch.tensor([user_idx] * len(batch_indices), dtype=torch.long).to(device)\n",
        "            item_tensor = batch_indices.to(device)\n",
        "            gmf_item_tensor = batch_gmf_features.to(device)\n",
        "            mlp_item_tensor = batch_mlp_features.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                scores = model(user_tensor, item_tensor, gmf_item_tensor, mlp_item_tensor)\n",
        "                all_scores.append(scores.cpu().numpy())\n",
        "\n",
        "        if all_scores:\n",
        "            all_scores = np.concatenate(all_scores)\n",
        "\n",
        "            # Get the indices of the top N scores\n",
        "            if len(all_scores) >= n_recommendations:\n",
        "                top_n_indices = np.argsort(all_scores)[-n_recommendations:][::-1]\n",
        "                recommended_items = [unseen_items[i] for i in top_n_indices]\n",
        "            else:\n",
        "                # If we have fewer items than requested recommendations\n",
        "                recommended_items = [unseen_items[i] for i in np.argsort(all_scores)[::-1]]\n",
        "\n",
        "            recommendations[user] = recommended_items\n",
        "        else:\n",
        "            print(f\"No scores computed for user {user}\")\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: MODEL TRAINING WITH EARLY STOPPING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Model parameters\n",
        "num_users = len(user_encoder.classes_)\n",
        "num_items = len(item_encoder.classes_)\n",
        "gmf_feature_dim = gmf_item_features.shape[1]\n",
        "mlp_feature_dim = mlp_item_features.shape[1]\n",
        "\n",
        "print(f\"Number of users: {num_users}\")\n",
        "print(f\"Number of items: {num_items}\")\n",
        "print(f\"GMF feature dimension: {gmf_feature_dim}\")\n",
        "print(f\"MLP feature dimension: {mlp_feature_dim}\")\n",
        "\n",
        "# Model hyperparameters\n",
        "EMBEDDING_DIM = 8\n",
        "MLP_DIMS = [32, 16, 8]\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 20\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "# Initialize model\n",
        "model = NeuMFPlusPlus(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    gmf_feature_dim=gmf_feature_dim,\n",
        "    mlp_feature_dim=mlp_feature_dim,\n",
        "    mlp_dims=MLP_DIMS\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Training loop with early stopping\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_aucs = []\n",
        "best_val_auc = 0\n",
        "patience_counter = 0\n",
        "MODEL_SAVE_DIR = \"models/\"\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "best_model_path = os.path.join(MODEL_SAVE_DIR, 'neumf_plus_plus_best.pth')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate\n",
        "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_metrics['loss'])\n",
        "    val_aucs.append(val_metrics['auc'])\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}\")\n",
        "    print(f\"Val F1 Score: {val_metrics['f1']:.4f}, Val AUC: {val_metrics['auc']:.4f}\")\n",
        "\n",
        "    # Early stopping based on validation AUC\n",
        "    if val_metrics['auc'] > best_val_auc:\n",
        "        best_val_auc = val_metrics['auc']\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Save the best model\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'num_users': num_users,\n",
        "            'num_items': num_items,\n",
        "            'gmf_feature_dim': gmf_feature_dim,\n",
        "            'mlp_feature_dim': mlp_feature_dim,\n",
        "            'embedding_dim': EMBEDDING_DIM,\n",
        "            'mlp_dims': MLP_DIMS,\n",
        "            'best_val_auc': best_val_auc\n",
        "        }, best_model_path)\n",
        "        print(f\"Saved best model with Val AUC: {best_val_auc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"Early stopping patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 9: MODEL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Try to load the best model, but continue with the current model if loading fails\n",
        "try:\n",
        "    checkpoint = torch.load(best_model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model with validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
        "except (FileNotFoundError, RuntimeError, pickle.UnpicklingError) as e:\n",
        "    print(f\"Could not load saved model: {e}\")\n",
        "    print(\"Continuing with current model state.\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
        "print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
        "print(f\"Test AUC: {test_metrics['auc']:.4f}\")\n",
        "\n",
        "# Evaluate on cold start items\n",
        "all_df = pd.concat([train_df, val_df, test_df])\n",
        "cold_start_metrics = evaluate_cold_start_items(\n",
        "    model=model,\n",
        "    all_df=all_df,\n",
        "    test_df=test_df,\n",
        "    gmf_item_features=gmf_item_features,\n",
        "    mlp_item_features=mlp_item_features,\n",
        "    min_interactions=5  # Define cold start items as those with <= 5 interactions\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 10: GENERATE RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Generate recommendations\n",
        "recommendations = generate_recommendations(\n",
        "    model=model,\n",
        "    user_encoder=user_encoder,\n",
        "    item_encoder=item_encoder,\n",
        "    all_df=all_df,\n",
        "    test_df=test_df,\n",
        "    gmf_item_features=gmf_item_features,\n",
        "    mlp_item_features=mlp_item_features,\n",
        "    n_recommendations=5\n",
        ")\n",
        "\n",
        "# Display sample recommendations\n",
        "print(\"\\nSample Recommendations:\")\n",
        "for i, (user, items) in enumerate(recommendations.items()):\n",
        "    if i >= 3:  # Just show 3 users for brevity\n",
        "        break\n",
        "    print(f\"\\nRecommendations for user {user}:\")\n",
        "    for item in items:\n",
        "        print(f\"Item: {item}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 11: PLOT TRAINING CURVES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# AUC curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_aucs, label='Validation AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Validation AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_curves.png\")\n",
        "plt.close()\n",
        "print(\"Training curves saved to training_curves.png\")\n",
        "\n",
        "print(\"\\nNeuMF++ with BERT and Autoencoders implementation complete!\")\n",
        "\n",
        "# Memory cleanup\n",
        "del train_dataset, val_dataset, test_dataset\n",
        "del train_loader, val_loader, test_loader\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q6NOAjv0gME",
        "outputId": "356d7209-d3d9-4f93-f9e1-2f681f3e7383"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 1: SETUP AND DATA LOADING\n",
            "================================================================================\n",
            "Using device: cuda\n",
            "Reviews shape: (1689188, 18)\n",
            "Metadata shape: (492009, 19)\n",
            "\n",
            "================================================================================\n",
            "STEP 2: DATA PREPROCESSING\n",
            "================================================================================\n",
            "Common ASINs: 61709\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-11885311706e>:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  reviews_df['interaction'] = (reviews_df['overall'] >= 3).astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interaction distribution (binary):\n",
            "interaction\n",
            "1    1436504\n",
            "0     183627\n",
            "Name: count, dtype: int64\n",
            "Unique users: 192395\n",
            "Unique items: 61709\n",
            "\n",
            "================================================================================\n",
            "STEP 3: LOAD PRECOMPUTED BERT EMBEDDINGS\n",
            "================================================================================\n",
            "Loading precomputed BERT embeddings from /content/drive/My Drive/bt4222data/embeddings/metadata_embeddings.npz\n",
            "Loaded BERT embeddings with shape: (498191, 384)\n",
            "Number of items with embeddings: 498191\n",
            "Number of items in metadata: 61709\n",
            "Warning: BERT embeddings count does not match metadata count. Adjusting...\n",
            "BERT embedding dimension: 384\n",
            "\n",
            "================================================================================\n",
            "STEP 4: AUTOENCODER FOR FEATURE EXTRACTION\n",
            "================================================================================\n",
            "Training GMF SDAE...\n",
            "Epoch [1/10], Loss: 0.003245\n",
            "Epoch [2/10], Loss: 0.002022\n",
            "Epoch [3/10], Loss: 0.001950\n",
            "Epoch [4/10], Loss: 0.001949\n",
            "Epoch [5/10], Loss: 0.001948\n",
            "Epoch [6/10], Loss: 0.001947\n",
            "Epoch [7/10], Loss: 0.001947\n",
            "Epoch [8/10], Loss: 0.001947\n",
            "Epoch [9/10], Loss: 0.001948\n",
            "Epoch [10/10], Loss: 0.001947\n",
            "Training MLP SDAE...\n",
            "Epoch [1/10], Loss: 0.002981\n",
            "Epoch [2/10], Loss: 0.002008\n",
            "Epoch [3/10], Loss: 0.001967\n",
            "Epoch [4/10], Loss: 0.001967\n",
            "Epoch [5/10], Loss: 0.001967\n",
            "Epoch [6/10], Loss: 0.001967\n",
            "Epoch [7/10], Loss: 0.001967\n",
            "Epoch [8/10], Loss: 0.001966\n",
            "Epoch [9/10], Loss: 0.001966\n",
            "Epoch [10/10], Loss: 0.001966\n",
            "GMF item features shape: (498191, 8)\n",
            "MLP item features shape: (498191, 8)\n",
            "\n",
            "================================================================================\n",
            "STEP 5: DATASET AND DATALOADER CREATION\n",
            "================================================================================\n",
            "Train size: 1134091\n",
            "Validation size: 243020\n",
            "Test size: 243020\n",
            "\n",
            "================================================================================\n",
            "STEP 6: MODEL DEFINITION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 7: TRAINING AND EVALUATION FUNCTIONS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 8: MODEL TRAINING WITH EARLY STOPPING\n",
            "================================================================================\n",
            "Number of users: 192395\n",
            "Number of items: 61709\n",
            "GMF feature dimension: 8\n",
            "MLP feature dimension: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1108/1108 [00:11<00:00, 93.43it/s, loss=0.309]\n",
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 95.28it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.3608\n",
            "Val Loss: 0.3280, Val Accuracy: 0.8867\n",
            "Val Precision: 0.8867, Val Recall: 1.0000\n",
            "Val F1 Score: 0.9399, Val AUC: 0.7005\n",
            "Saved best model with Val AUC: 0.7005\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1108/1108 [00:11<00:00, 92.47it/s, loss=0.295]\n",
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 87.82it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20\n",
            "Train Loss: 0.2980\n",
            "Val Loss: 0.3285, Val Accuracy: 0.8866\n",
            "Val Precision: 0.8868, Val Recall: 0.9996\n",
            "Val F1 Score: 0.9399, Val AUC: 0.7146\n",
            "Saved best model with Val AUC: 0.7146\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1108/1108 [00:12<00:00, 91.46it/s, loss=0.324]\n",
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 95.19it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20\n",
            "Train Loss: 0.2577\n",
            "Val Loss: 0.3536, Val Accuracy: 0.8854\n",
            "Val Precision: 0.8873, Val Recall: 0.9974\n",
            "Val F1 Score: 0.9391, Val AUC: 0.7080\n",
            "Early stopping patience: 1/3\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1108/1108 [00:11<00:00, 92.83it/s, loss=0.241] \n",
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 94.63it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20\n",
            "Train Loss: 0.2257\n",
            "Val Loss: 0.3976, Val Accuracy: 0.8825\n",
            "Val Precision: 0.8888, Val Recall: 0.9915\n",
            "Val F1 Score: 0.9373, Val AUC: 0.7005\n",
            "Early stopping patience: 2/3\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1108/1108 [00:11<00:00, 92.38it/s, loss=0.206]\n",
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 88.22it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20\n",
            "Train Loss: 0.2037\n",
            "Val Loss: 0.4495, Val Accuracy: 0.8537\n",
            "Val Precision: 0.9025, Val Recall: 0.9360\n",
            "Val F1 Score: 0.9190, Val AUC: 0.6970\n",
            "Early stopping patience: 3/3\n",
            "Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "STEP 9: MODEL EVALUATION\n",
            "================================================================================\n",
            "Could not load saved model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "Continuing with current model state.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 238/238 [00:02<00:00, 90.68it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "Test Loss: 0.4429\n",
            "Test Accuracy: 0.8536\n",
            "Test Precision: 0.9027\n",
            "Test Recall: 0.9358\n",
            "Test F1 Score: 0.9189\n",
            "Test AUC: 0.7005\n",
            "\n",
            "Evaluating model on cold start items...\n",
            "Found 4857 cold start items in the test set.\n",
            "Number of interactions with cold start items: 6583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 7/7 [00:00<00:00, 63.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cold Start Items Metrics:\n",
            "Loss: 0.5111\n",
            "Accuracy: 0.7957\n",
            "Precision: 0.8788\n",
            "Recall: 0.8845\n",
            "F1 Score: 0.8817\n",
            "AUC: 0.6485\n",
            "\n",
            "================================================================================\n",
            "STEP 10: GENERATE RECOMMENDATIONS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Generating recommendations: 100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Recommendations:\n",
            "\n",
            "Recommendations for user A15PUGYZ6C2IPU:\n",
            "Item: B008JCVF0U\n",
            "Item: B0044YPN0A\n",
            "Item: B000V5P90K\n",
            "Item: B000OG6I6A\n",
            "Item: B000053HC5\n",
            "\n",
            "Recommendations for user AKXT3E60ZZQCY:\n",
            "Item: B000UZH7P6\n",
            "Item: B00004TX71\n",
            "Item: B007HSKSP0\n",
            "Item: B00003G1RG\n",
            "Item: B000RZNI4S\n",
            "\n",
            "Recommendations for user ABHM4V3BH2C2T:\n",
            "Item: B0044YPN0A\n",
            "Item: B001BTCSI6\n",
            "Item: B000OG6I6A\n",
            "Item: B000V5P90K\n",
            "Item: B000053HC5\n",
            "\n",
            "================================================================================\n",
            "STEP 11: PLOT TRAINING CURVES\n",
            "================================================================================\n",
            "Training curves saved to training_curves.png\n",
            "\n",
            "NeuMF++ with BERT and Autoencoders implementation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfNXX5va0pao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}