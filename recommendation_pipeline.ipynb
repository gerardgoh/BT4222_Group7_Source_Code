{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Product Recommendation System Explanation\n",
        "\n",
        "##Overview:\n",
        "- This code implements a hybrid product recommendation system that combines several recommendation approaches:\n",
        "\n",
        "##Content-based filtering\n",
        "- Recommends products similar to a query based on product embeddings\n",
        "\n",
        "##Neural Collaborative Filtering (NCF)\n",
        "- A neural network model for user-item interaction prediction\n",
        "\n",
        "##SBERT-enhanced NCF\n",
        "- An advanced NCF model that incorporates textual information using SBERT embeddings\n",
        "\n",
        "##Item-based collaborative filtering\n",
        "- Recommends products similar to a given product"
      ],
      "metadata": {
        "id": "UceasiuwIIXy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3_PgA8il4s2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sys\n",
        "import traceback\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reGPO4unnWEM",
        "outputId": "851090d1-fa07-49fc-bc03-2bf0afe27249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NCF Model: A neural collaborative filtering model with:\n",
        "\n",
        "  - Separate embedding layers for GMF (Generalized Matrix Factorization) and MLP (Multi-Layer Perceptron) paths\n",
        "  - Multiple MLP layers with ReLU activations and dropout\n",
        "  - A final sigmoid activation for binary prediction"
      ],
      "metadata": {
        "id": "C5qiLv4NIMoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF8n7iFEl4s4"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Collaborative Filtering (NCF) model for binary interaction prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_users, n_items, factors=32, mlp_layers=[64, 32, 16], dropout=0.2):\n",
        "        super(NCF, self).__init__()\n",
        "\n",
        "        # GMF part\n",
        "        self.user_gmf_embedding = nn.Embedding(n_users, factors)\n",
        "        self.item_gmf_embedding = nn.Embedding(n_items, factors)\n",
        "\n",
        "        # MLP part\n",
        "        self.user_mlp_embedding = nn.Embedding(n_users, factors)\n",
        "        self.item_mlp_embedding = nn.Embedding(n_items, factors)\n",
        "\n",
        "        # MLP layers\n",
        "        self.mlp_layers = nn.ModuleList()\n",
        "        input_size = 2 * factors\n",
        "\n",
        "        for next_size in mlp_layers:\n",
        "            self.mlp_layers.append(nn.Linear(input_size, next_size))\n",
        "            self.mlp_layers.append(nn.ReLU())\n",
        "            self.mlp_layers.append(nn.Dropout(dropout))\n",
        "            input_size = next_size\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(factors + mlp_layers[-1], 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Embedding):\n",
        "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        # GMF part\n",
        "        user_gmf = self.user_gmf_embedding(user_id)\n",
        "        item_gmf = self.item_gmf_embedding(item_id)\n",
        "        gmf_vector = user_gmf * item_gmf\n",
        "\n",
        "        # MLP part\n",
        "        user_mlp = self.user_mlp_embedding(user_id)\n",
        "        item_mlp = self.item_mlp_embedding(item_id)\n",
        "        mlp_vector = torch.cat([user_mlp, item_mlp], dim=1)\n",
        "\n",
        "        for layer in self.mlp_layers:\n",
        "            mlp_vector = layer(mlp_vector)\n",
        "\n",
        "        # Concatenate GMF and MLP parts\n",
        "        concat_vector = torch.cat([gmf_vector, mlp_vector], dim=1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(concat_vector)\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NeuMFPlusPlus Model:\n",
        " - An enhanced NCF that incorporates Sentence-BERT embeddings for content representation.\n"
      ],
      "metadata": {
        "id": "A8Pk2zLEIPYc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFbbJw7Ul4s4"
      },
      "outputs": [],
      "source": [
        "class NeuMFPlusPlus(nn.Module):\n",
        "    def __init__(self, num_users, num_items, item_bert_dim,\n",
        "                 embedding_dim=64, mlp_dims=[128, 64, 32], dropout_rate=0.2):\n",
        "        super(NeuMFPlusPlus, self).__init__()\n",
        "\n",
        "        # GMF part\n",
        "        self.user_gmf_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_gmf_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # MLP part\n",
        "        self.user_mlp_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_mlp_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # Item content projection (Sentence-BERT embedding)\n",
        "        self.item_bert_projection = nn.Linear(item_bert_dim, embedding_dim)\n",
        "        self.bert_bn = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "        # MLP layers\n",
        "        mlp_input_dim = embedding_dim * 2 + embedding_dim  # user + item + bert\n",
        "        self.mlp_layers = nn.ModuleList()\n",
        "        self.mlp_batch_norms = nn.ModuleList()\n",
        "\n",
        "        # First layer\n",
        "        self.mlp_layers.append(nn.Linear(mlp_input_dim, mlp_dims[0]))\n",
        "        self.mlp_batch_norms.append(nn.BatchNorm1d(mlp_dims[0]))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(mlp_dims)-1):\n",
        "            self.mlp_layers.append(nn.Linear(mlp_dims[i], mlp_dims[i+1]))\n",
        "            self.mlp_batch_norms.append(nn.BatchNorm1d(mlp_dims[i+1]))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(mlp_dims[-1] + embedding_dim, 1)\n",
        "\n",
        "        # Activation and dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices, item_bert_emb):\n",
        "        # GMF part\n",
        "        user_gmf_emb = self.user_gmf_embedding(user_indices)\n",
        "        item_gmf_emb = self.item_gmf_embedding(item_indices)\n",
        "        gmf_output = user_gmf_emb * item_gmf_emb\n",
        "\n",
        "        # MLP part\n",
        "        user_mlp_emb = self.user_mlp_embedding(user_indices)\n",
        "        item_mlp_emb = self.item_mlp_embedding(item_indices)\n",
        "\n",
        "        # Process item Sentence-BERT embedding\n",
        "        item_bert_emb = self.item_bert_projection(item_bert_emb)\n",
        "        item_bert_emb = self.bert_bn(item_bert_emb)\n",
        "        item_bert_emb = self.relu(item_bert_emb)\n",
        "\n",
        "        # Concatenate user, item and BERT embeddings\n",
        "        mlp_input = torch.cat([user_mlp_emb, item_mlp_emb, item_bert_emb], dim=1)\n",
        "\n",
        "        # Apply MLP layers\n",
        "        for i, layer in enumerate(self.mlp_layers):\n",
        "            mlp_input = layer(mlp_input)\n",
        "            mlp_input = self.mlp_batch_norms[i](mlp_input)\n",
        "            mlp_input = self.relu(mlp_input)\n",
        "            mlp_input = self.dropout(mlp_input)\n",
        "\n",
        "        # Concatenate GMF and MLP parts\n",
        "        concat_output = torch.cat([gmf_output, mlp_input], dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        prediction = self.sigmoid(self.output_layer(concat_output))\n",
        "\n",
        "        return prediction.squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recommender Class:\n",
        " - The Recommender class serves as the main interface for generating recommendations. It loads and manages all models and data, and provides methods for each recommendation approach.\n",
        "\n",
        "#Main Methods\n",
        "\n",
        "- get_cb_recommendations(): Content-based recommendations using query embeddings\n",
        "- get_neumf_recommendations(): User-based recommendations using the NCF model\n",
        "- get_sbert_neumf_recommendations(): Enhanced user-based recommendations with SBERT\n",
        "- get_item_based_recommendations(): Recommendations based on item similarity\n",
        "- test_recommendations(): A comprehensive test function for all recommendation methods"
      ],
      "metadata": {
        "id": "D4eEHaTkIsvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li2RhXK9l4s5"
      },
      "outputs": [],
      "source": [
        "class Recommender:\n",
        "    def __init__(self, metadata_df, reviews_df, metadata_embeddings_path,\n",
        "                 ncf_model_path, ncf_encoders_path,\n",
        "                 query_embeddings=None, has_reviews=None, query_type=None,\n",
        "                 item_embeddings_path=None,\n",
        "                 sbert_model_path=None, sbert_encoders_path=None,\n",
        "                 sbert_embeddings_path=None):\n",
        "\n",
        "        self.metadata_df = metadata_df\n",
        "        self.reviews_df = reviews_df\n",
        "        self.query_type = query_type\n",
        "\n",
        "        # Load product embeddings for content-based filtering\n",
        "        loaded_data = np.load(metadata_embeddings_path, allow_pickle=True)\n",
        "        self.product_embeddings = loaded_data['embeddings']\n",
        "        self.product_asins = loaded_data['asins'].tolist()\n",
        "        self.product_metadata = {row['asin']: row.to_dict() for _, row in metadata_df.iterrows()}\n",
        "\n",
        "        # Set device for PyTorch models\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize and load binary NCF model\n",
        "        self.ncf_model, self.n_items = self._load_ncf_model(ncf_model_path)\n",
        "        self.user_encoder, self.item_encoder = self._load_encoders(ncf_encoders_path)\n",
        "        self.ncf_model = self.ncf_model.to(self.device)\n",
        "\n",
        "        # Initialize for item embeddings and query embeddings\n",
        "        self.item_embeddings_path = item_embeddings_path\n",
        "        self.query_embeddings = query_embeddings\n",
        "\n",
        "        with open(self.item_embeddings_path, 'rb') as f:\n",
        "            self.item_embeddings_data = pickle.load(f)\n",
        "\n",
        "        # Initialize for SBERT-enhanced NCF model\n",
        "        self.sbert_model = None\n",
        "        self.sbert_user_encoder = None\n",
        "        self.sbert_item_encoder = None\n",
        "        self.sbert_embeddings = None\n",
        "        self.asin_to_idx = None\n",
        "        self.n_sbert_items = 0\n",
        "\n",
        "        # Load SBERT model if provided\n",
        "        if sbert_model_path and sbert_encoders_path:\n",
        "            # Load encoders first to get item_bert_dim\n",
        "            sbert_encoders_data = self._load_sbert_encoders(sbert_encoders_path)\n",
        "            self.sbert_user_encoder = sbert_encoders_data.get('user_encoder')\n",
        "            self.sbert_item_encoder = sbert_encoders_data.get('item_encoder')\n",
        "            self.asin_to_idx = sbert_encoders_data.get('asin_to_idx')\n",
        "\n",
        "            # Load SBERT embeddings\n",
        "            self.sbert_embeddings = np.load(sbert_embeddings_path)\n",
        "            item_bert_dim = self.sbert_embeddings.shape[1]\n",
        "\n",
        "            # Load model with the correct item_bert_dim\n",
        "            self.sbert_model, self.n_sbert_items = self._load_sbert_model(\n",
        "                sbert_model_path,\n",
        "                item_bert_dim=item_bert_dim\n",
        "            )\n",
        "\n",
        "            self.sbert_model = self.sbert_model.to(self.device)\n",
        "\n",
        "    def _load_encoders(self, encoders_path):\n",
        "        \"\"\"Load encoders from a pickle file\"\"\"\n",
        "        with open(encoders_path, 'rb') as f:\n",
        "            encoders_data = pickle.load(f)\n",
        "\n",
        "        user_encoder = encoders_data.get('user_encoder')\n",
        "        item_encoder = encoders_data.get('item_encoder')\n",
        "\n",
        "        return user_encoder, item_encoder\n",
        "\n",
        "    def _load_sbert_encoders(self, encoders_path):\n",
        "        \"\"\"Load SBERT encoders from a pickle file\"\"\"\n",
        "        with open(encoders_path, 'rb') as f:\n",
        "            encoders_data = pickle.load(f)\n",
        "        return encoders_data\n",
        "\n",
        "    def _load_ncf_model(self, model_path):\n",
        "        torch.serialization.add_safe_globals([LabelEncoder])\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        checkpoint = torch.load(model_path, weights_only=False)\n",
        "\n",
        "        # Get basic model parameters\n",
        "        n_users = checkpoint['n_users']\n",
        "        n_items = checkpoint['n_items']\n",
        "\n",
        "        # Get embedding dimension (factors)\n",
        "        factors = checkpoint.get('factors', 32)\n",
        "\n",
        "        # Try to determine MLP layer architecture from the saved model\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "        # Find all linear layer weights for MLP\n",
        "        mlp_linear_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            if 'mlp_layers' in key and 'weight' in key:\n",
        "                layer_index = int(key.split('.')[1])\n",
        "                if layer_index % 3 == 0:  # Every third layer is linear (Linear, ReLU, Dropout pattern)\n",
        "                    mlp_linear_keys.append(key)\n",
        "\n",
        "        # Sort by layer index\n",
        "        mlp_linear_keys.sort(key=lambda x: int(x.split('.')[1]))\n",
        "\n",
        "        # Extract layer sizes\n",
        "        mlp_layers = []\n",
        "        for key in mlp_linear_keys:\n",
        "            weight = state_dict[key]\n",
        "            out_features = weight.shape[0]\n",
        "            mlp_layers.append(out_features)\n",
        "\n",
        "        if not mlp_layers:\n",
        "            # Default if we can't determine\n",
        "            mlp_layers = [64, 32, 16]\n",
        "\n",
        "        dropout = 0.2  # Default dropout rate\n",
        "\n",
        "        print(f\"Creating NCF model with: n_users={n_users}, n_items={n_items}, factors={factors}, mlp_layers={mlp_layers}\")\n",
        "\n",
        "        # Create model with the determined architecture\n",
        "        model = NCF(n_users, n_items, factors=factors, mlp_layers=mlp_layers, dropout=dropout)\n",
        "\n",
        "        # Load state dict\n",
        "        try:\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Successfully loaded model parameters\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading exact parameters: {e}\")\n",
        "            print(\"Attempting to load with strict=False\")\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Loaded model with non-strict parameter matching\")\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        return model, n_items\n",
        "\n",
        "    def _load_sbert_model(self, model_path, item_bert_dim=384):\n",
        "        \"\"\"Load the SBERT-enhanced NCF model\"\"\"\n",
        "        # Add safe globals for model loading\n",
        "        torch.serialization.add_safe_globals([LabelEncoder])\n",
        "\n",
        "        checkpoint = torch.load(model_path, weights_only=False)\n",
        "\n",
        "        num_users = checkpoint.get('num_users', 0)\n",
        "        num_items = checkpoint.get('num_items', 0)\n",
        "        embedding_dim = checkpoint.get('embedding_dim', 64)\n",
        "        mlp_dims = checkpoint.get('mlp_dims', [128, 64, 32])\n",
        "\n",
        "        model = NeuMFPlusPlus(\n",
        "            num_users=num_users,\n",
        "            num_items=num_items,\n",
        "            item_bert_dim=item_bert_dim,\n",
        "            embedding_dim=embedding_dim,\n",
        "            mlp_dims=mlp_dims\n",
        "        )\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        return model, num_items\n",
        "\n",
        "    def get_cb_recommendations(self, n=5):\n",
        "        \"\"\"Get content-based recommendations based on query embeddings\"\"\"\n",
        "        similarity_scores = cosine_similarity(\n",
        "            self.query_embeddings.reshape(1, -1),\n",
        "            self.product_embeddings\n",
        "        )[0]\n",
        "\n",
        "        top_n = similarity_scores.argsort()[-n:][::-1]\n",
        "\n",
        "        # Convert indices to ASINs\n",
        "        recommendations = []\n",
        "        for idx in top_n:\n",
        "            asin = self.product_asins[idx]\n",
        "            if asin in self.product_metadata:\n",
        "                recommendations.append(asin)\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_neumf_recommendations(self, user_id, n=5):\n",
        "        \"\"\"Get recommendations using the binary NCF model\"\"\"\n",
        "        try:\n",
        "            user = self.user_encoder.transform([user_id])[0]\n",
        "        except Exception as e:\n",
        "            print(f\"User {user_id} not found in binary NCF training data: {e}\")\n",
        "            return []\n",
        "\n",
        "        user_tensor = torch.tensor([user] * self.n_items, dtype=torch.long).to(self.device)\n",
        "        item_tensor = torch.tensor(list(range(self.n_items)), dtype=torch.long).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = self.ncf_model(user_tensor, item_tensor)\n",
        "\n",
        "        # Get top N items\n",
        "        _, indices = torch.topk(predictions, n)\n",
        "\n",
        "        # Map indices back to ASINs\n",
        "        recommendations = []\n",
        "        for idx in indices:\n",
        "            item_idx = idx.item()\n",
        "            try:\n",
        "                asin = self.item_encoder.inverse_transform([item_idx])[0]\n",
        "                recommendations.append(asin)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting item index {item_idx} to ASIN: {e}\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_sbert_neumf_recommendations(self, user_id, n=5):\n",
        "        \"\"\"Get recommendations using the SBERT enhanced NCF model\"\"\"\n",
        "        try:\n",
        "            user = self.sbert_user_encoder.transform([user_id])[0]\n",
        "        except Exception as e:\n",
        "            print(f\"User {user_id} not found in SBERT training data: {e}\")\n",
        "            return []\n",
        "\n",
        "        # Get all items\n",
        "        all_items = range(self.n_sbert_items)\n",
        "\n",
        "        # Create tensors for user and items\n",
        "        user_tensor = torch.tensor([user] * len(all_items), dtype=torch.long).to(self.device)\n",
        "        item_tensor = torch.tensor(list(all_items), dtype=torch.long).to(self.device)\n",
        "\n",
        "        # Get item SBERT embeddings\n",
        "        item_embeddings_tensor = torch.tensor(self.sbert_embeddings, dtype=torch.float).to(self.device)\n",
        "\n",
        "        # Get predictions in batches to avoid memory issues\n",
        "        batch_size = 1024\n",
        "        all_scores = []\n",
        "\n",
        "        for i in range(0, len(all_items), batch_size):\n",
        "            batch_user = user_tensor[i:i+batch_size]\n",
        "            batch_items = item_tensor[i:i+batch_size]\n",
        "            batch_embeddings = item_embeddings_tensor[i:i+batch_size]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_predictions = self.sbert_model(batch_user, batch_items, batch_embeddings)\n",
        "                all_scores.append(batch_predictions.cpu())\n",
        "\n",
        "        # Combine all predictions\n",
        "        all_scores = torch.cat(all_scores)\n",
        "\n",
        "        # Get top N items\n",
        "        _, indices = torch.topk(all_scores, n)\n",
        "\n",
        "        # Map indices back to ASINs\n",
        "        recommendations = []\n",
        "        for idx in indices:\n",
        "            item_idx = idx.item()\n",
        "            try:\n",
        "                asin = self.sbert_item_encoder.inverse_transform([item_idx])[0]\n",
        "                recommendations.append(asin)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting SBERT item index {item_idx} to ASIN: {e}\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_item_based_recommendations(self, item_id, n=5):\n",
        "        \"\"\"Get item-based collaborative filtering recommendations\"\"\"\n",
        "        item_to_idx = self.item_embeddings_data['item_to_idx']\n",
        "        idx_to_item = self.item_embeddings_data['idx_to_item']\n",
        "        embeddings = self.item_embeddings_data['embeddings']\n",
        "\n",
        "        # Get item embedding and calculate similarities\n",
        "        item_idx = item_to_idx[item_id]\n",
        "        item_embedding = embeddings[item_idx].reshape(1, -1)\n",
        "\n",
        "        # Calculate similarity with all items\n",
        "        similarities = cosine_similarity(item_embedding, embeddings)[0]\n",
        "\n",
        "        # Get top similar items\n",
        "        similar_indices = similarities.argsort()[-(n+1):][::-1]\n",
        "\n",
        "        # Remove the item itself\n",
        "        similar_indices = [idx for idx in similar_indices if idx != item_idx][:n]\n",
        "\n",
        "        # Convert to ASINs\n",
        "        recommendations = [idx_to_item[idx] for idx in similar_indices]\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def test_recommendations(self, test_user_id=None, test_item_id=None, n=5):\n",
        "        results = {}\n",
        "\n",
        "        # 1. Test content-based recommendations\n",
        "        print(\"\\n=== Testing Content-Based Recommendations ===\")\n",
        "        try:\n",
        "            cb_recommendations = self.get_cb_recommendations(n=n)\n",
        "            print(f\"Content-based recommendations: {cb_recommendations}\")\n",
        "            results['content_based'] = cb_recommendations\n",
        "        except Exception as e:\n",
        "            print(f\"Error in content-based recommendations: {str(e)}\")\n",
        "            results['content_based'] = f\"Error: {str(e)}\"\n",
        "\n",
        "        # 2. Test NCF recommendations\n",
        "        print(\"\\n=== Testing NCF Recommendations ===\")\n",
        "        try:\n",
        "            # If no test user provided, find one from the encoder\n",
        "            if test_user_id is None:\n",
        "                if hasattr(self, 'user_encoder') and self.user_encoder is not None:\n",
        "                    try:\n",
        "                        # Try to get first user from encoder classes\n",
        "                        test_user_id = self.user_encoder.classes_[0]\n",
        "                        print(f\"Using first available user: {test_user_id}\")\n",
        "                    except:\n",
        "                        # Fall back to first user in reviews\n",
        "                        test_user_id = self.reviews_df['reviewerID'].iloc[0]\n",
        "                        print(f\"Using first user from reviews: {test_user_id}\")\n",
        "\n",
        "            ncf_recommendations = self.get_neumf_recommendations(test_user_id, n=n)\n",
        "            print(f\"NCF recommendations for user {test_user_id}: {ncf_recommendations}\")\n",
        "            results['ncf'] = ncf_recommendations\n",
        "        except Exception as e:\n",
        "            print(f\"Error in NCF recommendations: {str(e)}\")\n",
        "            results['ncf'] = f\"Error: {str(e)}\"\n",
        "\n",
        "        # 3. Test SBERT-enhanced recommendations\n",
        "        print(\"\\n=== Testing SBERT-Enhanced Recommendations ===\")\n",
        "        try:\n",
        "            # Use same test user as NCF if possible\n",
        "            sbert_recommendations = self.get_sbert_neumf_recommendations(test_user_id, n=n)\n",
        "            print(f\"SBERT recommendations for user {test_user_id}: {sbert_recommendations}\")\n",
        "            results['sbert'] = sbert_recommendations\n",
        "        except Exception as e:\n",
        "            print(f\"Error in SBERT recommendations: {str(e)}\")\n",
        "            results['sbert'] = f\"Error: {str(e)}\"\n",
        "\n",
        "        # 4. Test item-based recommendations\n",
        "        print(\"\\n=== Testing Item-Based Recommendations ===\")\n",
        "        try:\n",
        "            # If no test item provided, find one from the item embeddings\n",
        "            if test_item_id is None:\n",
        "                if hasattr(self, 'item_embeddings_data') and self.item_embeddings_data is not None:\n",
        "                    try:\n",
        "                        # Try to get first item from item_to_idx\n",
        "                        test_item_id = list(self.item_embeddings_data['item_to_idx'].keys())[0]\n",
        "                        print(f\"Using first available item: {test_item_id}\")\n",
        "                    except:\n",
        "                        # Fall back to first item in reviews\n",
        "                        test_item_id = self.reviews_df['asin'].iloc[0]\n",
        "                        print(f\"Using first item from reviews: {test_item_id}\")\n",
        "\n",
        "            item_recommendations = self.get_item_based_recommendations(test_item_id, n=n)\n",
        "            print(f\"Item-based recommendations for item {test_item_id}: {item_recommendations}\")\n",
        "            results['item_based'] = item_recommendations\n",
        "        except Exception as e:\n",
        "            print(f\"Error in item-based recommendations: {str(e)}\")\n",
        "            results['item_based'] = f\"Error: {str(e)}\"\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n=== Test Summary ===\")\n",
        "        for method, recs in results.items():\n",
        "            status = \"✓ Success\" if not isinstance(recs, str) and recs else \"✗ Failed\"\n",
        "            print(f\"{method}: {status}\")\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Code"
      ],
      "metadata": {
        "id": "AoC8-UJCJ1wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGElvZhJl4s6",
        "outputId": "b65cc2ea-9568-469b-af65-410bccd8990a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading metadata and reviews...\n",
            "Metadata shape: (498191, 13)\n",
            "Reviews shape: (1689188, 18)\n",
            "\n",
            "Generating query embedding for 'wireless headphones'...\n",
            "Generated query embedding with shape: (384,)\n",
            "\n",
            "Initializing recommender...\n",
            "Loading model from /content/drive/MyDrive/bt4222data/final_models/ncf_binary_model.pt\n",
            "Creating NCF model with: n_users=192403, n_items=63001, factors=64, mlp_layers=[128, 64, 32]\n",
            "Successfully loaded model parameters\n",
            "\n",
            "=== Testing Recommendations for user A15PUGYZ6C2IPU with query 'wireless headphones' ===\n",
            "\n",
            "1. Content-based recommendations for 'wireless headphones':\n",
            "Found 5 content-based recommendations:\n",
            "  1. Wireless Headphones (ASIN: B00409LKJC)\n",
            "  2. Wireless Behind-the-Head Stereo Headphones (Blue/Silver) (ASIN: B002JQ6NWM)\n",
            "  3. 5 IN 1 Wireless Headphones (ASIN: B00579L24W)\n",
            "  4. Digital 007 4-in-1 Wireless Headphones (ASIN: B0045DNX48)\n",
            "  5. Wireless Headphones (ASIN: B0002198MS)\n",
            "\n",
            "2. NCF recommendations for user A15PUGYZ6C2IPU:\n",
            "Found 5 NCF recommendations:\n",
            "  1. Manfrotto 055XPROB Pro Tripod Legs (Black) (ASIN: B000UMX7FI)\n",
            "  2. Peerless PRGS-UNV Precision Gear Universal Projector Mount - Black (ASIN: B000TXNS6G)\n",
            "  3. Canon EF 70-200mm f/4L USM Telephoto Zoom Lens for Canon SLR Cameras (ASIN: B000053HH5)\n",
            "  4. Case Logic TBC-312 Pocket Video Camcorder Case with Storage (Black) (ASIN: B001S0PWWC)\n",
            "  5. Toshiba HD-A20 1080p HD DVD Player (ASIN: B000MKC34E)\n",
            "\n",
            "3. SBERT-enhanced recommendations for user A15PUGYZ6C2IPU:\n",
            "Found 5 SBERT-enhanced recommendations:\n",
            "  1. Arctic Silver 5 Thermal Compound 3.5 Grams (ASIN: B000OGX5AM)\n",
            "  2. Intel Core i5-2500K Quad-Core Processor 3.3 GHz 6 MB Cache LGA 1155 - BX80623I52500K (ASIN: B004EBUXHQ)\n",
            "  3. Mediabridge ULTRA Series Subwoofer Cable (15 Feet) - Dual Shielded with Gold Plated RCA to RCA Connectors - White (ASIN: B003FVVMS0)\n",
            "  4. Neewer 110CM 43-Inch 5-in-1 Collapsible Multi-Disc Light Reflector (ASIN: B002ZIMEMW)\n",
            "  5. Cable Matters Cat6 Snagless Ethernet Patch Cable in Black 25 Feet (ASIN: B007NZGPAY)\n",
            "\n",
            "✅ Testing completed!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    # Set paths to your data and model files\n",
        "    DATA_DIR = \"/content/drive/MyDrive/bt4222data\"\n",
        "\n",
        "    metadata_path = os.path.join(DATA_DIR, \"Meta Data Cleaned\", \"meta_data_cleaned.csv.zip\")\n",
        "    reviews_path = os.path.join(DATA_DIR,\"Reviews Data Cleaned\", \"cleaned_reviews.csv\")\n",
        "    metadata_embeddings_path = os.path.join(DATA_DIR, \"embeddings\", \"metadata_embeddings.npz\")\n",
        "    ncf_model_path = os.path.join(DATA_DIR, \"final_models\", \"ncf_binary_model.pt\")\n",
        "    ncf_encoders_path = os.path.join(DATA_DIR, \"final_models\", \"encoders.pkl\")\n",
        "    item_embeddings_path = os.path.join(DATA_DIR, \"embeddings\", \"item_embeddings.pkl\")\n",
        "    sbert_model_path = os.path.join(DATA_DIR, \"sbert_models\", \"neumf_sbert_model.pt\")\n",
        "    sbert_encoders_path = os.path.join(DATA_DIR, \"sbert_models\", \"sbert_encoders.pkl\")\n",
        "    sbert_embeddings_path = os.path.join(DATA_DIR, \"sbert_models\", \"embeddings\", \"item_sbert_embeddings.npy\")\n",
        "\n",
        "    try:\n",
        "        print(\"Loading metadata and reviews...\")\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "        reviews_df = pd.read_csv(reviews_path)\n",
        "\n",
        "        print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "        print(f\"Reviews shape: {reviews_df.shape}\")\n",
        "\n",
        "        # Generate query embedding for \"wireless headphones\"\n",
        "        print(\"\\nGenerating query embedding for 'wireless headphones'...\")\n",
        "        try:\n",
        "            # First try to load a sentence transformer model for embedding\n",
        "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            query_embeddings = model.encode(\"wireless headphones\")\n",
        "            print(f\"Generated query embedding with shape: {query_embeddings.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error using SentenceTransformer: {e}\")\n",
        "\n",
        "            # Fallback: load product embeddings and find products with \"wireless headphones\" in title\n",
        "            loaded_data = np.load(metadata_embeddings_path, allow_pickle=True)\n",
        "            product_embeddings = loaded_data['embeddings']\n",
        "            product_asins = loaded_data['asins'].tolist()\n",
        "\n",
        "            # Find products with \"wireless headphones\" in title\n",
        "            matching_indices = []\n",
        "            for i, asin in enumerate(product_asins):\n",
        "                product = metadata_df[metadata_df['asin'] == asin]\n",
        "                if not product.empty and 'title' in product.columns:\n",
        "                    title = product['title'].iloc[0].lower()\n",
        "                    if 'wireless' in title and 'headphone' in title:\n",
        "                        matching_indices.append(i)\n",
        "\n",
        "            if matching_indices:\n",
        "                # Use the average embedding of matching products\n",
        "                matching_embeddings = [product_embeddings[i] for i in matching_indices]\n",
        "                query_embeddings = np.mean(matching_embeddings, axis=0)\n",
        "                print(f\"Using average embedding of {len(matching_indices)} wireless headphone products\")\n",
        "            else:\n",
        "                # Just use the first product embedding as a fallback\n",
        "                query_embeddings = product_embeddings[0]\n",
        "                print(\"Using fallback product embedding\")\n",
        "\n",
        "        # Initialize recommender with specific parameters\n",
        "        print(\"\\nInitializing recommender...\")\n",
        "        recommender = Recommender(\n",
        "            metadata_df=metadata_df,\n",
        "            reviews_df=reviews_df,\n",
        "            metadata_embeddings_path=metadata_embeddings_path,\n",
        "            ncf_model_path=ncf_model_path,\n",
        "            ncf_encoders_path=ncf_encoders_path,\n",
        "            query_embeddings=query_embeddings,\n",
        "            item_embeddings_path=item_embeddings_path,\n",
        "            sbert_model_path=sbert_model_path,\n",
        "            sbert_encoders_path=sbert_encoders_path,\n",
        "            sbert_embeddings_path=sbert_embeddings_path\n",
        "        )\n",
        "\n",
        "        # Test with specific user ID and query\n",
        "        user_id = \"A15PUGYZ6C2IPU\"\n",
        "        print(f\"\\n=== Testing Recommendations for user {user_id} with query 'wireless headphones' ===\")\n",
        "\n",
        "        # Get content-based recommendations\n",
        "        print(\"\\n1. Content-based recommendations for 'wireless headphones':\")\n",
        "        cb_recommendations = recommender.get_cb_recommendations(n=5)\n",
        "\n",
        "        # Display recommendations with product details\n",
        "        if cb_recommendations:\n",
        "            print(f\"Found {len(cb_recommendations)} content-based recommendations:\")\n",
        "            for i, asin in enumerate(cb_recommendations):\n",
        "                product = metadata_df[metadata_df['asin'] == asin]\n",
        "                if not product.empty and 'title' in product.columns:\n",
        "                    title = product['title'].iloc[0]\n",
        "                    print(f\"  {i+1}. {title} (ASIN: {asin})\")\n",
        "                else:\n",
        "                    print(f\"  {i+1}. ASIN: {asin} (metadata not found)\")\n",
        "        else:\n",
        "            print(\"No content-based recommendations found\")\n",
        "\n",
        "        # Get NCF recommendations for specific user\n",
        "        print(f\"\\n2. NCF recommendations for user {user_id}:\")\n",
        "        ncf_recommendations = recommender.get_neumf_recommendations(user_id, n=5)\n",
        "\n",
        "        if ncf_recommendations:\n",
        "            print(f\"Found {len(ncf_recommendations)} NCF recommendations:\")\n",
        "            for i, asin in enumerate(ncf_recommendations):\n",
        "                product = metadata_df[metadata_df['asin'] == asin]\n",
        "                if not product.empty and 'title' in product.columns:\n",
        "                    title = product['title'].iloc[0]\n",
        "                    print(f\"  {i+1}. {title} (ASIN: {asin})\")\n",
        "                else:\n",
        "                    print(f\"  {i+1}. ASIN: {asin} (metadata not found)\")\n",
        "        else:\n",
        "            print(\"No NCF recommendations found\")\n",
        "\n",
        "        # Get SBERT-enhanced recommendations for specific user\n",
        "        print(f\"\\n3. SBERT-enhanced recommendations for user {user_id}:\")\n",
        "        sbert_recommendations = recommender.get_sbert_neumf_recommendations(user_id, n=5)\n",
        "\n",
        "        if sbert_recommendations:\n",
        "            print(f\"Found {len(sbert_recommendations)} SBERT-enhanced recommendations:\")\n",
        "            for i, asin in enumerate(sbert_recommendations):\n",
        "                product = metadata_df[metadata_df['asin'] == asin]\n",
        "                if not product.empty and 'title' in product.columns:\n",
        "                    title = product['title'].iloc[0]\n",
        "                    print(f\"  {i+1}. {title} (ASIN: {asin})\")\n",
        "                else:\n",
        "                    print(f\"  {i+1}. ASIN: {asin} (metadata not found)\")\n",
        "        else:\n",
        "            print(\"No SBERT-enhanced recommendations found\")\n",
        "\n",
        "        # Success message\n",
        "        print(\"\\n✅ Testing completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}